{
  ".\\main.py": {
    "file_title": "main.py",
    "file_path": ".\\main.py",
    "file_code": "from tkinter import ttk, Tk, Label, Frame, Text\nfrom tkinter.ttk import Button, Style\nfrom ttkthemes import ThemedTk\nimport subprocess\nimport uuid\nimport sys\nimport os\n\nfrom features.projects.projects_interface import ProjectTab\nclass SessionManager:\n    def __init__(self):\n        self.sessions = {}\n\n    def create_new_session(self):\n        session_id = str(uuid.uuid4())\n        self.sessions[session_id] = None\n        return session_id\n\n    def remove_session(self, session_id):\n        if session_id in self.sessions:\n            del self.sessions[session_id]\n\n    def is_valid_session(self, session_id):\n        return session_id in self.sessions\n\n\ndef close_tab(tab_control, index):\n    frame = tab_control.nametowidget(tab_control.tabs()[index])\n    tab_control.forget(frame)\n\n\ndef create_tab(tab_control, session_manager, feature):\n    tab = Frame(tab_control)\n    tab_control.add(tab, text=feature)\n    tab_control.select(tab)\n\n    close_button = Button(tab, text='X', command=lambda: close_tab(tab_control, tab_control.index(tab)))\n    close_button.pack(anchor=\"nw\")  # changed from 'ne' to 'nw'\n\n    if feature == \"Query\":\n        txt = Text(tab)\n        txt.insert('1.0', \"Output of Query\")\n    elif feature == \"mySystem\":\n        txt = Text(tab)\n        txt.insert('1.0', \"Output of mySystem\")\n    elif feature == \"Converse\":\n        txt = Text(tab)\n        txt.insert('1.0', \"Output of Converse\")\n    elif feature == \"Project\":\n        project_tab = ProjectTab(tab)\n\n\n\ndef main():\n    session_manager = SessionManager()\n    FEATURES = [\"Projects\", \"Query\", \"Converse\",\"mySystem\" ]\n\n    root = ThemedTk(theme=\"arc\")  # Set the theme to \"arc\"\n    root.state('zoomed')\n\n    button_frame = Frame(root)\n    button_frame.pack(pady=20)\n\n    add_button = Button(button_frame,text=\"Add Widget\")\n    add_button.grid(row=0, column=len(FEATURES), sticky='ew')\n    add_button.grid_remove()\n\n    tab_control = ttk.Notebook(root)\n    tab_control.pack(expand=1, fill=\"both\")\n\n    for index, feature in enumerate(FEATURES):\n        Button(button_frame,text=feature,command=lambda feature=feature: create_tab(tab_control, session_manager, feature)).grid(row=0, column=index, sticky='ew')\n        button_frame.grid_columnconfigure(index, weight=1)\n\n    root.mainloop()\n\nif __name__ == \"__main__\":\n    main()",
    "file_summary": "I'm sorry, but it seems you didn't provide the content of the Python file. Please provide the valid script for me to summarize its contents."
  },
  ".\\__init__.py": {
    "file_title": "__init__.py",
    "file_path": ".\\__init__.py",
    "file_code": "",
    "file_summary": "Sorry, I couldn't summarize the content of a Python file because you didn't provide either the file or its content. Could you please provide us with the Python code or the file? That way, I'll be able to describe its contents more accurately."
  },
  ".\\.misc\\KB_microservice\\kb_microservice.py": {
    "file_title": "kb_microservice.py",
    "file_path": ".\\.misc\\KB_microservice\\kb_microservice.py",
    "file_code": "import os\nimport flask\nimport logging\nimport json\nimport yaml\nimport threading\nfrom flask import request\nimport openai\nfrom time import time, sleep\n\n\n\nlog = logging.getLogger('werkzeug')\nlog.setLevel(logging.ERROR)\napp = flask.Flask('KB Articles')\n\n\n\n###     file operations\n\n\n\ndef save_yaml(filepath, data):\n    with open(filepath, 'w', encoding='utf-8') as file:\n        yaml.dump(data, file, allow_unicode=True)\n\n\n\ndef open_yaml(filepath):\n    with open(filepath, 'r', encoding='utf-8') as file:\n        data = yaml.load(file, Loader=yaml.FullLoader)\n    return data\n\n\n\ndef save_file(filepath, content):\n    with open(filepath, 'w', encoding='utf-8') as outfile:\n        outfile.write(content)\n\n\n\ndef open_file(filepath):\n    with open(filepath, 'r', encoding='utf-8', errors='ignore') as infile:\n        return infile.read()\n\n\n\n###     chatbot functions\n\n\n\n#def chatbot(messages, model=\"gpt-4-0613\", temperature=0):\ndef chatbot(messages, model=\"gpt-3.5-turbo-0613\", temperature=0):\n    openai.api_key = open_file('key_openai.txt')\n    max_retry = 7\n    retry = 0\n    while True:\n        try:\n            response = openai.ChatCompletion.create(model=model, messages=messages, temperature=temperature)\n            text = response['choices'][0]['message']['content']\n            return text, response['usage']['total_tokens']\n        except Exception as oops:\n            print(f'\\n\\nError communicating with OpenAI: \"{oops}\"')\n            if 'maximum context length' in str(oops):\n                a = messages.pop(1)\n                print('\\n\\n DEBUG: Trimming oldest message')\n                continue\n            retry += 1\n            if retry >= max_retry:\n                print(f\"\\n\\nExiting due to excessive errors in API: {oops}\")\n                exit(1)\n            print(f'\\n\\nRetrying in {2 ** (retry - 1) * 5} seconds...')\n            sleep(2 ** (retry - 1) * 5)\n\n\n\n###     KB functions\n\n\n\ndef update_directory():\n    kb_dir = 'kb/'\n    directory = ''\n    for filename in os.listdir(kb_dir):\n        if filename.endswith('.yaml'):\n            filepath = os.path.join(kb_dir, filename)\n            kb = open_yaml(filepath)\n            directory += '\\n%s - %s - %s - %s\\n' % (filename, kb['title'], kb['description'], kb['keywords'])\n    save_file('directory.txt', directory.strip())\n\n\n\ndef search_kb(query):\n    directory = open_file('directory.txt')\n    system = open_file('system_search.txt').replace('<<DIRECTORY>>', directory)\n    messages = [{'role': 'system', 'content': system}, {'role': 'user', 'content': query}]\n    response, tokens = chatbot(messages)\n    return json.loads(response)\n\n\n\ndef create_article(text):\n    system = open_file('system_create.txt')\n    messages = [{'role': 'system', 'content': system}, {'role': 'user', 'content': text}]\n    response, tokens = chatbot(messages)  # response should be JSON string\n    kb = json.loads(response)\n    save_yaml('kb/%s.yaml' % kb['title'], kb)\n    print('CREATE', kb['title'])\n\n\n\ndef update_article(payload):\n    kb = open_yaml('kb/%s.yaml' % payload['title'])\n    json_str = json.dumps(kb, indent=2)\n    system = open_file('system_update.txt').replace('<<KB>>', json_str)\n    messages = [{'role': 'system', 'content': system}, {'role': 'user', 'content': payload['input']}]\n    response, tokens = chatbot(messages)  # response should be JSON string\n    kb = json.loads(response)\n    save_yaml('kb/%s.yaml' % kb['title'], kb)\n    print('UPDATE', kb['title'])\n\n\n\n###     flask routes\n\n\n\n@app.route('/search', methods=['post'])\ndef search_endpoint():\n    update_directory()\n    payload = request.json  # payload should be {\"query\": \"{query}\"}\n    print(payload)\n    files = search_kb(payload['query'])  # this will always be a list of files, though it may be empty\n    result = list()\n    for f in files:\n        data = open_yaml(f'kb/{f}')\n        result.append(data)\n    return flask.Response(json.dumps(result), mimetype='application/json')\n\n\n\n@app.route('/create', methods=['post'])\ndef create_endpoint():\n    payload = request.json  # payload should be {\"input\": \"{text}\"}\n    threading.Thread(target=create_article, args=(payload['input'],)).start()\n    return flask.Response(json.dumps({\"status\": \"success\"}), mimetype='application/json')\n\n\n\n@app.route('/update', methods=['post'])\ndef update_endpoint():\n    payload = request.json  # payload should be {\"title\": \"{KB title to update}\", \"input\": \"{text}\"}\n    threading.Thread(target=update_article, args=(payload,)).start()\n    return flask.Response(json.dumps({\"status\": \"success\"}), mimetype='application/json')\n\n\n\nif __name__ == '__main__':\n    app.run(host='0.0.0.0', port=999)",
    "file_summary": "Apologies for the inconvenient misunderstanding, but I can't provide an analysis as you didn't provide any specific content from a Python file. To obtain helpful information, please provide the exact content or specific functions that need to be analyzed and summarized."
  },
  ".\\.misc\\KB_microservice\\test_kb_service.py": {
    "file_title": "test_kb_service.py",
    "file_path": ".\\.misc\\KB_microservice\\test_kb_service.py",
    "file_code": "import requests\nimport json\nfrom pprint import pprint as pp\n\n\n\ndef test_create_endpoint():\n    text = input(\"Enter the text for the new KB article: \")\n    payload = {\"input\": text}\n    response = requests.post(\"http://localhost:999/create\", json=payload)\n    print('\\n\\n\\n', response.json())\n\n\n\ndef test_search_endpoint():\n    query = input(\"Enter the search query: \")\n    payload = {\"query\": query}\n    response = requests.post(\"http://localhost:999/search\", json=payload)\n    print('\\n\\n\\n')\n    pp(response.json())\n\n\n\ndef test_update_endpoint():\n    title = input(\"Enter the title of the KB article to update: \")\n    text = input(\"Enter the new text for the KB article: \")\n    payload = {\"title\": title, \"input\": text}\n    response = requests.post(\"http://localhost:999/update\", json=payload)\n    print('\\n\\n\\n', response.json())\n\n\n\ndef main():\n    while True:\n        print(\"\\n\\n\\n1. Create KB article\")\n        print(\"2. Search KB articles\")\n        print(\"3. Update KB article\")\n        print(\"4. Exit\")\n        choice = input(\"\\n\\nEnter your choice: \")\n        if choice == '1':\n            test_create_endpoint()\n        elif choice == '2':\n            test_search_endpoint()\n        elif choice == '3':\n            test_update_endpoint()\n        elif choice == '4':\n            break\n        else:\n            print(\"\\n\\n\\nInvalid choice. Please enter a number between 1 and 4.\")\n\n\n\n\nif __name__ == \"__main__\":\n    main()",
    "file_summary": "I'm sorry, but there's no actual Python file or content provided in your query. Please provide the content or context of the Python file, so I can give you a brief summary of it."
  },
  ".\\.misc\\terminial\\goal_decomposition.py": {
    "file_title": "goal_decomposition.py",
    "file_path": ".\\.misc\\terminial\\goal_decomposition.py",
    "file_code": "import os\nimport sys\nsys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..')))\n\nfrom Core.gpt import GPT\n\nclass GoalDecomposer:\n    def __init__(self):\n        # Instantiate a GPT object to interact with OpenAI's GPT model\n        self.gpt = GPT()\n\n    def decompose_goal(self, goal):\n        # Set up the prompt for the GPT model to get sub-tasks for the given goal\n        prompt = f\"Given a goal '{goal}', please break it down into sub-tasks:\"\n\n        # Execute the GPT model using the prompt and obtain its output\n        response_text = self.gpt.generate_text(prompt)\n\n        # Split the obtained output into individual sub-tasks\n        sub_tasks_string = response_text.strip().split(\"\\n\")\n        \n        # Remove empty strings after splitting\n        sub_tasks = [task.strip() for task in sub_tasks_string if task.strip()]\n\n        return sub_tasks\n\nif __name__ == \"__main__\":\n    decomposer = GoalDecomposer()\n    test_goal = \"Install a new software\"\n    print(f\"Sub-tasks for the goal '{test_goal}':\")\n    sub_tasks = decomposer.decompose_goal(test_goal)\n    for sub_task in sub_tasks:\n        print(f\" - {sub_task}\")",
    "file_summary": "I'm sorry, but you haven't provided any specifics about the Python file or its content. Please provide details like the structure or the content of the Python file for me to summarize it."
  },
  ".\\.misc\\terminial\\terminal.py": {
    "file_title": "terminal.py",
    "file_path": ".\\.misc\\terminial\\terminal.py",
    "file_code": "import os\nimport sys\nsys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..')))\n\nfrom Core.gpt import GPT\nfrom goal_decomposition import GoalDecomposer\nimport os\nimport subprocess\nfrom Core.gpt import GPT\nfrom goal_decomposition import GoalDecomposer\n\nclass TaskExecutor:\n    def __init__(self):\n        self.gpt = GPT()\n        self.decomposer = GoalDecomposer()\n\n    def think_and_execute(self, task):\n        sub_tasks = self.decomposer.decompose_goal(task)\n        \n        try:\n            for sub_task in sub_tasks:\n                self.execute_sub_task(sub_task)\n                print(f\"The task '{sub_task}' was successfully executed.\")\n        except Exception as e:\n            print(f\"Error executing task '{sub_task}': {e}\")\n\n    def execute_sub_task(self, sub_task):\n        prompt = f\"Write a PowerShell command or script to accomplish the following task: {sub_task}\"\n        response = self.gpt.generate_text(prompt)\n        code = response.strip()\n        \n        print(f\"Thalis generated the following code for the task '{sub_task}':\\n{code}\\n\")\n\n        temp_script_file = \"terminal_custom_script.ps1\"\n\n        with open(temp_script_file, \"w\") as script_file:\n            script_file.write(code)\n\n        os.chmod(temp_script_file, 0o755)\n\n        # Execute the script and capture any errors\n        process = subprocess.Popen(f'powershell.exe -ExecutionPolicy Bypass -NoProfile -NonInteractive -File {temp_script_file}', shell=True, stderr=subprocess.PIPE, stdout=subprocess.PIPE)\n        stdout, stderr = process.communicate()\n\n        if process.returncode != 0:\n            raise Exception(f\"Execution failed. Error message: {stderr.decode().strip()}\")\n        else:\n            print(f\"Successfully executed the task. Output: {stdout.decode().strip()}\")\n\nos.system(\"\")\n\nif __name__ == \"__main__\":\n    task_executor = TaskExecutor()\n    user_task = input(\"Enter a task: \")\n    task_executor.think_and_execute(user_task)",
    "file_summary": "I'm so sorry, as your input is missing actual Python code or content. I would need proper code to give the brief summary of the code contents. Please provide the actual Python code, not just \"{content}\". This way, I will be able to summarize the file's content and functionality."
  },
  ".\\features\\__init__.py": {
    "file_title": "__init__.py",
    "file_path": ".\\features\\__init__.py",
    "file_code": "",
    "file_summary": "Sorry, but you haven't provided any specific Python file or its contents for me to summarise. Could you please provide the file or its content so I can assist you better?"
  },
  ".\\features\\converse\\converse_plugins\\conve_gpt.py": {
    "file_title": "conve_gpt.py",
    "file_path": ".\\features\\converse\\converse_plugins\\conve_gpt.py",
    "file_code": "import os\nimport openai\nfrom dotenv import load_dotenv\nload_dotenv()\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\ndef chat_gpt(model_name, messages):\n    completion = openai.ChatCompletion.create(\n        model=model_name,\n        messages=messages\n    )\n\n    return completion.choices[0].message['content']\n\ndef engage_gpt():\n    model_choice = input(\"\\nEnter the model: gpt-3.5-turbo-16k or gpt-4: \")\n    while model_choice not in [\"gpt-3.5-turbo-16k\", \"gpt-4\"]:\n        print(\"\\nInvalid model choice. Try again.\")\n        model_choice = input(\"\\nEnter the model: gpt-3.5-turbo-16k or gpt-4: \")\n\n    messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n    while True:\n        message = input(\"\\nUser: \")\n        if message.lower() in ['quit', 'exit', 'q', 'e']:\n            break\n\n        messages.append({\"role\": \"user\", \"content\": message})\n        response = chat_gpt(model_choice, messages)\n        print(f\"\\n{model_choice}: {response}\")\n        messages.append({\"role\": \"assistant\", \"content\": response})",
    "file_summary": "I'm sorry, but I can't provide a brief summary of the file at the moment because you haven't provided the actual content of the Python file. Could you please provide the code that's in the Python file, so I can analyze it?"
  },
  ".\\features\\converse\\converse_plugins\\__init__.py": {
    "file_title": "__init__.py",
    "file_path": ".\\features\\converse\\converse_plugins\\__init__.py",
    "file_code": "",
    "file_summary": "Apologies, but I can't provide a summary as you didn't provide the actual content of the Python file. Please replace '{file}' and '{content}' with the name of the file and the actual code respectively."
  },
  ".\\features\\mySystem\\mySystem_interface.py": {
    "file_title": "mySystem_interface.py",
    "file_path": ".\\features\\mySystem\\mySystem_interface.py",
    "file_code": "#when tab clicked open this\n\n#inside thab add button called \"system_cleaner\"\n\n",
    "file_summary": "I'm sorry, but I am unable to provide a summary without the specific content of the Python file. Once you provide the content, I'll be more than happy to help analyze it!"
  },
  ".\\features\\mySystem\\mySystem_plugins\\self_improve\\self_improvement.py": {
    "file_title": "self_improvement.py",
    "file_path": ".\\features\\mySystem\\mySystem_plugins\\self_improve\\self_improvement.py",
    "file_code": "import json\nimport os\nimport openai\nimport pathlib\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nclass GPTInterpreter:\n    def __init__(self):\n        self.api_key = os.getenv(\"OPENAI_API_KEY\")\n        openai.api_key = self.api_key\n\n    def interpret(self, user_data: str) -> str:\n        try:\n            response = openai.ChatCompletion.create(\n                model=\"gpt-4\",\n                messages=[\n                    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n                    {\"role\": \"user\", \"content\": user_data}\n                ]\n            )\n            return response.choices[0].message['content']\n        except Exception as e:\n            print(f\"Exception caught while trying to interpret user data: {e}\")\n            return None\n\n\n\ndef generate_tree_structure(path=pathlib.Path(__file__).parent.absolute(), level=0):\n    result = \"\"\n    for item in path.iterdir():\n        indent = \" \" * (level * 4) + \"|-- \"\n        if item.is_file() or item.name == '.env':\n            rel_path = str(item.resolve().relative_to(path))\n            result += indent + rel_path.replace(\"\\\\\", \"/\") + \"\\n\"\n        elif not \"__pycache__\" in str(item):\n            result += indent + str(item.name) + \"\\n\"\n            result += generate_tree_structure(item, level + 1)\n    return result\n\n\n\ndef update_source_code_map():\n    tree_output = generate_tree_structure().replace(\"\\\\\", \"/\")\n\n    handbook_file_path = os.path.join(os.path.dirname(__file__), 'handbook.json')\n    if not os.path.isfile(handbook_file_path):\n        with open(handbook_file_path, \"w\") as file:\n            file.write('{}') \n\n    with open(handbook_file_path, \"r+\") as file:\n        try:\n            data = json.load(file)\n        except Exception as e:\n            print(f\"Exception caught while trying to load JSON file: {e}\")\n            return None\n\n        data[\"source_code_map\"] = tree_output\n\n        file.seek(0)\n\n        try:\n            json.dump(data, file, indent=2)\n        except Exception as e:\n            print(f\"Exception caught while trying to dump JSON data: {e}\")   \n\n\nclass SelfImprove:\n    def __init__(self, gpt_interpreter: GPTInterpreter):\n        self.gpt_interpreter = gpt_interpreter\n\n    def map_source_code(self):\n        update_source_code_map()\n        print(\"\\nSource code map has been updated.\")\n\n    def analyze_source_code(self, input_value: str):\n        if input_value.lower() == 's':\n            source_files = []\n            for root, dirs, files in os.walk(\".\"):\n                if root == \"./__pycache__\":\n                    continue\n                for file in files:\n                    if file.endswith(\".py\") and file != \"source-code_labeled.json\" and file != \"handbook.json\":\n                        source_files.append(os.path.join(root, file))\n            total_files = len(source_files)\n            source_data = {}\n\n            with open(\"source-code_labeled.json\", \"w\") as f:\n                for index, file in enumerate(source_files, start=1):\n                    with open(file, \"r\") as reader:\n                        content = reader.read()\n\n                    print(f\"Analyzing file {file} ({index}/{total_files})\")\n                    summary = self.gpt_interpreter.interpret(\"Please provide a brief summary of the contents of the Python file '{file}' with content:\\n{content}\")\n\n                    source_data[file] = {\n                        \"file_title\": os.path.basename(file),\n                        \"file_path\": file,\n                        \"file_code\": content,\n                        \"file_summary\": summary\n                    }\n\n                json.dump(source_data, f, indent=2)\n\n            print(\"\\nSource code analysis has been completed.\")\n        else:\n            print(\"\\nUsing the existing 'source-code_labeled.json' file.\")\n\n    def get_self_improvement_instructions(self) -> str:\n        instructions = input(\"\\nPlease provide instructions for Thalis' self-improvement: \")\n        return instructions\n\n    def find_relevant_files(self, instructions: str):\n        data = None\n        with open(\"source-code_labeled.json\", \"r\") as f:\n            data = json.load(f)\n\n        relevant_files = []\n        for key, value in data.items():\n            relevance = self.gpt_interpreter.interpret({\n                \"action\": \"evaluate_relevance\",\n                \"file_summary\": value[\"file_summary\"],\n                \"instructions\": instructions,\n            })\n            if relevance.lower() == \"true\":\n                relevant_files.append(value[\"file_path\"])\n\n        base_dir = os.path.dirname(os.path.abspath(__file__))\n        file_path = os.path.join(base_dir, \"handbook.json\")\n        \n        with open(file_path, \"r+\") as file:\n            handbook_data = json.load(file)\n            handbook_data[\"relevant_files\"] = relevant_files\n            file.seek(0)      \n            json.dump(handbook_data, file, indent=2)\n\n        print(\"\\nRelevant files have been identified. Here is the list of the files:\")\n        for file in relevant_files:\n            print(f\"- {file}\")\n        print(\"\\nUpdated the 'relevant_files' section in the 'handbook.json' file.\")\n\n    def improve(self):\n        self.map_source_code()\n        strategy = input(\"\\nWould you like to analyze the source code again or use the existing 'source-code_labeled.json'? (Answer with 's' for scan and 'e' for existing): \")\n        self.analyze_source_code(strategy)\n        instructions = self.get_self_improvement_instructions()\n        self.find_relevant_files(instructions)\n\nif __name__ == \"__main__\":\n    gpt_interpreter_instance = GPTInterpreter()\n    self_improvement_instance = SelfImprove(gpt_interpreter=gpt_interpreter_instance)\n    self_improvement_instance.improve()\n",
    "file_summary": "I'm sorry, but as an AI, I can't physically read a file. However, I can analyze provided snippets of Python code. If you can paste the code in your question, I'd be more than happy to assist you!"
  },
  ".\\features\\mySystem\\mySystem_plugins\\self_improve\\__init__.py": {
    "file_title": "__init__.py",
    "file_path": ".\\features\\mySystem\\mySystem_plugins\\self_improve\\__init__.py",
    "file_code": "",
    "file_summary": "I'm sorry but you didn't provide me with relevant content from the Python file, could you please provide me with that?"
  },
  ".\\features\\mySystem\\mySystem_plugins\\system_cleaner\\space_cleaner.py": {
    "file_title": "space_cleaner.py",
    "file_path": ".\\features\\mySystem\\mySystem_plugins\\system_cleaner\\space_cleaner.py",
    "file_code": "import os\nimport shutil\n\n# Get the current script directory\ncurrent_dir = os.path.dirname(os.path.realpath(__file__))\n\n# Walk up the directory structure until we reach the \"Thalis 30\" directory\nmain_dir = current_dir\nwhile os.path.basename(main_dir) != \"Thalis 30\":\n    main_dir = os.path.dirname(main_dir)\n\n# Delete any \"__pycache__\" directories\nfor root_dir, dirs, _ in os.walk(main_dir):\n    if \"__pycache__\" in dirs:\n        pycache_path = os.path.join(root_dir, \"__pycache__\")\n        shutil.rmtree(pycache_path)\n        print(f\"Deleted __pycache__ directory in {root_dir}\")",
    "file_summary": "Sorry, but you didn't provide the content of the Python file. For me to provide a summary of the file, you first need to provide the contents of the '{file}'."
  },
  ".\\features\\projects\\projects_interface.py": {
    "file_title": "projects_interface.py",
    "file_path": ".\\features\\projects\\projects_interface.py",
    "file_code": "from tkinter import Canvas, Entry, Frame, Text, Toplevel, Tk\nimport tkinter as tk\nfrom tkinter.ttk import Button, Style\n\nclass MoveableFrame(Frame):\n    def __init__(self, master=None, **kwargs):\n        super().__init__(master, **kwargs)\n        self.grid_propagate(0)\n\n        self.bind(\"<ButtonPress-1>\", self.start_move)\n        self.bind(\"<ButtonRelease-1>\", self.stop_move)\n        self.bind(\"<B1-Motion>\", self.do_move)\n\n        self.text_widget = Text(self, height=5, width=20)\n        self.text_widget.pack(side=\"left\", fill=\"both\")\n        self.text_widget.bind(\"<ButtonPress-1>\", self.start_move)\n        self.text_widget.bind(\"<ButtonRelease-1>\", self.stop_move)\n        self.text_widget.bind(\"<B1-Motion>\", self.do_move)\n\n        self.close_button = Button(self, text=\"Close\", command=self.close)\n        self.close_button.pack(side=\"left\")\n\n        self.send_button = Button(self, text=\"Send\", command=self.send)  \n        self.send_button.pack(side=\"left\") \n\n    def start_move(self, event):\n        self.x = event.x\n        self.y = event.y\n\n    def stop_move(self, event):\n        self.x = None\n        self.y = None\n\n    def do_move(self, event):\n        delta_x = event.x - self.x\n        delta_y = event.y - self.y\n        x = self.winfo_x() + delta_x\n        y = self.winfo_y() + delta_y\n        self.place(x=x, y=y)\n\n    def close(self):\n        self.destroy()\n\n    def send(self):  # Renamed pin to send\n        pass\n\nclass ProjectTab:\n    def __init__(self, parent):\n        self.parent = parent\n        self.canvas = Canvas(parent)\n        self.canvas.pack(fill=\"both\", expand=True)\n        \n        self.style = Style()\n        self.style.configure('Add.TButton', font=('calibri', 11, 'bold'), borderwidth='1') \n\n        self.add_button = Button(self.canvas, text=\"Add Widget\", command=self.add_widget, style='Add.TButton')\n        self.add_button.pack(anchor=\"nw\")\n\n    def add_widget(self):\n        widget = self.create_widget()\n        widget.place(x=100, y=100)\n\n    def create_widget(self):\n        text_box = MoveableFrame(self.canvas, width=200, height=100, bg='#1A1C23')\n        text_box.text_widget.delete(1.0, \"end\")\n        text_box.text_widget.insert(1.0, \"Describe the widget\")\n        return text_box\n\nclass MessageInputPopup(Toplevel):\n    def __init__(self, title, callback):\n        super().__init__()\n        self.title(title)\n        self.geometry(\"+{}+{}\".format(self.winfo_screenwidth() // 2, self.winfo_screenheight() // 2))\n        self.callback = callback\n        self.entry = Entry(self)\n        self.entry.pack(side=\"top\", fill=\"Close\")\n        self.entry.bind('<Return>', self.send)\n        self.entry.focus_set()\n\n    def send(self, event=None):\n        self.callback(self.entry.get())\n        self.destroy()\n\ndef main():\n    root = Tk()\n    root.state('zoomed')\n    button_frame = Frame(root)\n    button_frame.pack(pady=20)\n    ProjectTab(button_frame)\n    root.mainloop()\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_summary": "I'm sorry, but you didn't give any specific Python file or its content. Please share the needed Python file or its code so I can summarize its content for you."
  },
  ".\\features\\projects\\__init__.py": {
    "file_title": "__init__.py",
    "file_path": ".\\features\\projects\\__init__.py",
    "file_code": "",
    "file_summary": "I'm sorry, I can't furnish a summary because the content of the Python file '{file}' has not been provided. Please provide the Python code and I'll be able to give a brief summary of it."
  },
  ".\\features\\projects\\projects_plugins\\__init__.py": {
    "file_title": "__init__.py",
    "file_path": ".\\features\\projects\\projects_plugins\\__init__.py",
    "file_code": "",
    "file_summary": "I'm sorry, but without being able to review the actual Python file or its content, I'm unable to provide a brief summary of its contents. Could you please provide the content of the file?"
  },
  ".\\features\\projects\\projects_plugins\\VectorDB Memory\\constants.py": {
    "file_title": "constants.py",
    "file_path": ".\\features\\projects\\projects_plugins\\VectorDB Memory\\constants.py",
    "file_code": "import os\nfrom dotenv import load_dotenv\nfrom chromadb.config import Settings\n\nload_dotenv()\n\n# Define the folder for storing database\nPERSIST_DIRECTORY = os.environ.get('PERSIST_DIRECTORY')\n\n# Define the Chroma settings\nCHROMA_SETTINGS = Settings(\n        chroma_db_impl='duckdb+parquet',\n        persist_directory=PERSIST_DIRECTORY,\n        anonymized_telemetry=False\n)\n",
    "file_summary": "Apologies for the misunderstanding, but in order to provide a brief summary of the contents of a Python file, the specific content or code from the file is required. The placeholders \"{file}\" and \"{content}\" are not actual content. Please provide the actual Python code you want to discuss, so I can provide a meaningful summary."
  },
  ".\\features\\projects\\projects_plugins\\VectorDB Memory\\ingest.py": {
    "file_title": "ingest.py",
    "file_path": ".\\features\\projects\\projects_plugins\\VectorDB Memory\\ingest.py",
    "file_code": "import os\nimport glob\nfrom typing import List\nfrom dotenv import load_dotenv\nfrom multiprocessing import Pool\nfrom tqdm import tqdm\n\nfrom langchain.document_loaders import (\n    CSVLoader,\n    EverNoteLoader,\n    PyMuPDFLoader,\n    TextLoader,\n    UnstructuredEmailLoader,\n    UnstructuredEPubLoader,\n    UnstructuredHTMLLoader,\n    UnstructuredMarkdownLoader,\n    UnstructuredODTLoader,\n    UnstructuredPowerPointLoader,\n    UnstructuredWordDocumentLoader,\n)\n\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain.vectorstores import Chroma\nfrom langchain.embeddings import HuggingFaceEmbeddings\nfrom langchain.docstore.document import Document\nfrom constants import CHROMA_SETTINGS\n\n\nload_dotenv()\n\n\n#\u00c2\u00a0Load environment variables\npersist_directory = os.environ.get('PERSIST_DIRECTORY')\nsource_directory = os.environ.get('SOURCE_DIRECTORY', 'source_documents')\nembeddings_model_name = os.environ.get('EMBEDDINGS_MODEL_NAME')\nchunk_size = 500\nchunk_overlap = 50\n\n\n# Custom document loaders\nclass MyElmLoader(UnstructuredEmailLoader):\n    \"\"\"Wrapper to fallback to text/plain when default does not work\"\"\"\n\n    def load(self) -> List[Document]:\n        \"\"\"Wrapper adding fallback for elm without html\"\"\"\n        try:\n            try:\n                doc = UnstructuredEmailLoader.load(self)\n            except ValueError as e:\n                if 'text/html content not found in email' in str(e):\n                    # Try plain text\n                    self.unstructured_kwargs[\"content_source\"]=\"text/plain\"\n                    doc = UnstructuredEmailLoader.load(self)\n                else:\n                    raise\n        except Exception as e:\n            # Add file_path to exception message\n            raise type(e)(f\"{self.file_path}: {e}\") from e\n\n        return doc\n\n\n# Map file extensions to document loaders and their arguments\nLOADER_MAPPING = {\n    \".csv\": (CSVLoader, {}),\n    # \".docx\": (Docx2txtLoader, {}),\n    \".doc\": (UnstructuredWordDocumentLoader, {}),\n    \".docx\": (UnstructuredWordDocumentLoader, {}),\n    \".enex\": (EverNoteLoader, {}),\n    \".eml\": (MyElmLoader, {}),\n    \".epub\": (UnstructuredEPubLoader, {}),\n    \".html\": (UnstructuredHTMLLoader, {}),\n    \".md\": (UnstructuredMarkdownLoader, {}),\n    \".odt\": (UnstructuredODTLoader, {}),\n    \".pdf\": (PyMuPDFLoader, {}),\n    \".ppt\": (UnstructuredPowerPointLoader, {}),\n    \".pptx\": (UnstructuredPowerPointLoader, {}),\n    \".txt\": (TextLoader, {\"encoding\": \"utf8\"}),\n    # Add more mappings for other file extensions and loaders as needed\n}\n\n\ndef load_single_document(file_path: str) -> List[Document]:\n    ext = \".\" + file_path.rsplit(\".\", 1)[-1]\n    if ext in LOADER_MAPPING:\n        loader_class, loader_args = LOADER_MAPPING[ext]\n        loader = loader_class(file_path, **loader_args)\n        return loader.load()\n\n    raise ValueError(f\"Unsupported file extension '{ext}'\")\n\ndef load_documents(source_dir: str, ignored_files: List[str] = []) -> List[Document]:\n    \"\"\"\n    Loads all documents from the source documents directory, ignoring specified files\n    \"\"\"\n    all_files = []\n    for ext in LOADER_MAPPING:\n        all_files.extend(\n            glob.glob(os.path.join(source_dir, f\"**/*{ext}\"), recursive=True)\n        )\n    filtered_files = [file_path for file_path in all_files if file_path not in ignored_files]\n\n    with Pool(processes=os.cpu_count()) as pool:\n        results = []\n        with tqdm(total=len(filtered_files), desc='Loading new documents', ncols=80) as pbar:\n            for i, docs in enumerate(pool.imap_unordered(load_single_document, filtered_files)):\n                results.extend(docs)\n                pbar.update()\n\n    return results\n\ndef process_documents(ignored_files: List[str] = []) -> List[Document]:\n    \"\"\"\n    Load documents and split in chunks\n    \"\"\"\n    print(f\"Loading documents from {source_directory}\")\n    documents = load_documents(source_directory, ignored_files)\n    if not documents:\n        print(\"No new documents to load\")\n        exit(0)\n    print(f\"Loaded {len(documents)} new documents from {source_directory}\")\n    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n    texts = text_splitter.split_documents(documents)\n    print(f\"Split into {len(texts)} chunks of text (max. {chunk_size} tokens each)\")\n    return texts\n\ndef does_vectorstore_exist(persist_directory: str) -> bool:\n    \"\"\"\n    Checks if vectorstore exists\n    \"\"\"\n    if os.path.exists(os.path.join(persist_directory, 'index')):\n        if os.path.exists(os.path.join(persist_directory, 'chroma-collections.parquet')) and os.path.exists(os.path.join(persist_directory, 'chroma-embeddings.parquet')):\n            list_index_files = glob.glob(os.path.join(persist_directory, 'index/*.bin'))\n            list_index_files += glob.glob(os.path.join(persist_directory, 'index/*.pkl'))\n            # At least 3 documents are needed in a working vectorstore\n            if len(list_index_files) > 3:\n                return True\n    return False\n\ndef main():\n    # Create embeddings\n    from langchain.embeddings import OpenAIEmbeddings\n    embeddings = OpenAIEmbeddings() \n\n    if does_vectorstore_exist(persist_directory):\n        # Update and store locally vectorstore\n        print(f\"Appending to existing vectorstore at {persist_directory}\")\n        db = Chroma(persist_directory=persist_directory, embedding_function=embeddings, client_settings=CHROMA_SETTINGS)\n        collection = db.get()\n        texts = process_documents([metadata['source'] for metadata in collection['metadatas']])\n        print(f\"Creating embeddings. May take some minutes...\")\n        db.add_documents(texts)\n    else:\n        # Create and store locally vectorstore\n        print(\"Creating new vectorstore\")\n        texts = process_documents()\n        print(f\"Creating embeddings. May take some minutes...\")\n        db = Chroma.from_documents(texts, embeddings, persist_directory=persist_directory, client_settings=CHROMA_SETTINGS)\n    db.persist()\n    db = None\n\n    print(f\"Ingestion complete! You can now run thalis.py\")\n\n\nif __name__ == \"__main__\":\n    main()\n",
    "file_summary": "Apologies, but you haven't provided any specific Python file or its content to summarize. Please provide the necessary details so I can assist you more effectively."
  },
  ".\\features\\projects\\projects_plugins\\VectorDB Memory\\thalis.py": {
    "file_title": "thalis.py",
    "file_path": ".\\features\\projects\\projects_plugins\\VectorDB Memory\\thalis.py",
    "file_code": "import os\nfrom dotenv import load_dotenv\nfrom langchain.vectorstores import Chroma\nfrom langchain.embeddings import OpenAIEmbeddings\nfrom langchain.chat_models import ChatOpenAI\nfrom langchain.chains import RetrievalQA\nimport argparse\n\nload_dotenv()\n\nopenai_api_key = os.environ.get(\"OPENAI_API_KEY\")\npersist_directory = os.environ.get(\"PERSIST_DIRECTORY\")\nembedding = OpenAIEmbeddings()\n\ndb = Chroma(persist_directory=persist_directory, embedding_function=embedding)\nretriever = db.as_retriever(search_kwargs={\"k\": 2})\n\nturbo_llm = ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\")\n\nqa_chain = RetrievalQA.from_chain_type(\n    llm=turbo_llm, chain_type=\"stuff\", retriever=retriever, return_source_documents=True\n)\n\ndef process_llm_response(llm_response):\n    print(llm_response[\"result\"])\n    print(\"\\n\\nSources:\")\n    for source in llm_response[\"source_documents\"]:\n        print(source.metadata[\"source\"])\n\ndef main():\n    args = parse_arguments()\n\n    while True:\n        query = input(\"\\nEnter a query: \")\n        if query == \"exit\":\n            break\n        if query.strip() == \"\":\n            continue\n\n        llm_response = qa_chain(query)\n        process_llm_response(llm_response)\n\ndef parse_arguments():\n    parser = argparse.ArgumentParser(\n        description=\"DocumentsGPT: Interact with your documents using OpenAI's API instead of local language models.\"\n    )\n    return parser.parse_args()\n\nif __name__ == \"__main__\":\n    main()",
    "file_summary": "Sorry, but your query seems to be formatted incorrectly. I need the actual Python file content (\"content\") to provide a summary of it. Please replace \"{content}\" with the Python code you want summarized."
  },
  ".\\features\\query\\query_plugins\\query_chains\\f0_ui_terminal.py": {
    "file_title": "f0_ui_terminal.py",
    "file_path": ".\\features\\query\\query_plugins\\query_chains\\f0_ui_terminal.py",
    "file_code": "import json\nimport sys\nimport os\n\nsys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\n\nfrom core.f1_pilot import start_query\nfrom features.introspection.guided_introspection import guided_introspection\nfrom features.converse.conversational_gpt import engage_gpt\n\n\ndef ui_terminal():\n    while True:\n        print(\"\\nWelcome to Thalis!\")\n        print(\"Choose an option:\")\n        print(\"1. Query\")\n        print(\"2. Guided introspection\")\n        print(\"3. Converse with GPT\")\n        print(\"4. - Exit\")\n\n        while True:  # Add loop\n            try:\n                user_choice = int(input(\"\\nYour choice (Number): \"))\n                if user_choice not in [1, 2, 3, 4]:  # Check if input is among the valid choices                   \n                    raise ValueError(\"Invalid option. Please choose from the available options.\")\n                break  # If input is valid, break the loop to proceed.\n            except ValueError as e:\n                print(e)  # Print the error message\n        if user_choice == 1:\n            start_query()\n        elif user_choice == 2:\n            guided_introspection()\n        elif user_choice == 3:\n            engage_gpt()\n        elif user_choice == 4:\n            # Instead of breaking, print a goodbye message and quit the application completely.\n            print(\"Thank you for using Thalis. Goodbye!\")\n            sys.exit()\n\n\nif __name__ == \"__main__\":\n    ui_terminal()\n",
    "file_summary": "I'm sorry, but you haven't provided the actual content of the Python file. Could you provide the content so that I can present a summary?"
  },
  ".\\features\\query\\query_plugins\\query_chains\\f1_pilot.py": {
    "file_title": "f1_pilot.py",
    "file_path": ".\\features\\query\\query_plugins\\query_chains\\f1_pilot.py",
    "file_code": "import json\nimport sys\nimport os\nimport json\nos.chdir(os.path.dirname(os.path.abspath(__file__)))\nfrom f3_memory_management import status_memory, read_memory\nfrom f2_handbook_management import process_handbook\n\ndef initialize_system():\n    # \\\\\\ log_process() \\\\\\ #\n    \n    return status_memory()\n\ndef log_process():\n    # Log the process in logger.json\n    pass\n\ndef start_query():\n    # ------------ user_input = input(\"\\nPlease enter your query: \")\n    print(\"\\nStart query\")\n    user_input = \"write a poem about flowers and save it in a file called flowers.txt please\"\n\n    #~~~~process_handbook(user_input) \n    \n    #initialize_system()    \n\n    # print(\"do you want to start a new query or load old memory?\")\n    # user_input = input(\" type n or m: \")\n\n\n    # if user_input == \"n\"\n    #   memory(status)=\"inactive\"\n    #   initialize_system()\n    \n\n    # if user_input == \"m\"\n    #   print(\"Search for the memory or write the ID of the memory: \")\n    #   print(\"type search or ID\")?\n\n    #   if user_input=\"search\":  \n    #       print(\"Here is a list of the memories:\")  \n    #       print memories with details\n    #       print(\"submit the ID of the memory you want to load:\") \n    #       load selected memory to memory.json \n    #       memory(status)=\"display work\"\n    #       initialize_system()\n\n    #   if user_input=\"ID\":\n    #   load selected memory to memory.json     \n    #   memory(status)=\"display work\"\n    #   initialize_system()\n\n\n    # elif:\n    #   print(\"wrong input, type n or m:\")\n    #   user_input = input(\" \")",
    "file_summary": "I'm sorry, but your message is missing the content of the Python file. Please provide the code you want me to analyze."
  },
  ".\\features\\query\\query_plugins\\query_chains\\f2_handbook_management.py": {
    "file_title": "f2_handbook_management.py",
    "file_path": ".\\features\\query\\query_plugins\\query_chains\\f2_handbook_management.py",
    "file_code": "import json\nimport sys\nimport os\nimport json\nimport os\nos.chdir(os.path.dirname(os.path.abspath(__file__)))\n\n\ndef process_handbook(user):\n    from f3_memory_management import status_memory, update_memory\n    with open(\"context.txt\", \"r\") as context_file:\n        context = context_file.read()\n\n    with open(\"handbook.json\", \"r\") as handbook_file:\n        handbook_data = json.load(handbook_file)\n\n    handbook_data[\"context\"] = context\n    handbook_data[\"user\"] = user\n\n    handbook_data[\"prompt\"] = context + \". \u00c2\u00ac \" + user\n\n\n    with open(\"handbook.json\", \"w\") as handbook_file:\n        json.dump(handbook_data, handbook_file)\n\n    \n    print(\"\\nHandbook processed\")\n    target = [\"status\",\"objective\"]\n    update_memory(target) # update status\n    \n    return status_memory\n\ndef read_handbook(variable):\n    with open(\"handbook.json\", \"r\") as file:\n        variable_data = json.load(file)\n    \n    variable = variable_data[variable]\n    return variable\n\n\n\n",
    "file_summary": "I'm sorry, I cannot give you any brief summary as you have not provided Python file '{file}' and its '{content}'. Please provide specific Python file with its content."
  },
  ".\\features\\query\\query_plugins\\query_chains\\f3_memory_management.py": {
    "file_title": "f3_memory_management.py",
    "file_path": ".\\features\\query\\query_plugins\\query_chains\\f3_memory_management.py",
    "file_code": "import json\nimport sys\nimport os\nimport json\nos.chdir(os.path.dirname(os.path.abspath(__file__)))\n\n\ndef status_memory():\n    status = json.load(open(\"memory.json\"))[\"status\"]\n\n    if status == \"inactive\": # unused?\n        from f2_handbook_management import process_handbook\n        return process_handbook()\n\n    elif status == \"objective\":\n        from f4_trajectory_and_actions import objective\n        return objective()\n\n    elif status == \"dossier\":\n        from core.f4_trajectory_and_actions import dossier\n        return dossier()\n        \n    \n    elif status == \"gpt_instructions\":\n        from core.f6_user_interaction import gpt_instructions\n        return print(\"Halt\") #gpt_instructions()  # function / template\n\n    elif status == \"terminal_instructions\":\n        from core.f6_user_interaction import terminal_instructions\n        return terminal_instructions()  # function / template\n\n    elif status == \"display_work\":\n        from core.f6_user_interaction import display_work\n        return display_work()\n    \n    elif status == \"user_feedback\":\n        from core.f6_user_interaction import user_feedback\n        return user_feedback()    \n\n    elif status == \"execute_work\":\n        from core.f4_trajectory_and_actions import execute_work\n        return execute_work()\n\n    elif status == \"modify_work\":\n        from core.f4_trajectory_and_actions import modify_work\n        return modify_work()\n    \n    elif status == \"archive_memory\":\n        return archive_memory()\n\n    elif status == \"execute_memory\":\n        return execute_memory()\n\n\ndef update_memory(target):\n    with open(\"memory.json\", \"r\") as file:\n        memory_data = json.load(file)\n    memory_data[target[0]] = target[1]\n    with open(\"memory.json\", \"w\") as file:\n        json.dump(memory_data, file)    \n\n    print(\"\\nMemory updated: \", target)\n    return status_memory()\n\ndef read_memory(variable):\n    with open(\"memory.json\", \"r\") as file:\n        memory_data = json.load(file)\n    memory = memory_data[variable]\n    return memory\n\n\ndef archive_memory():\n    print(\"Do you want to save or discard memory.json? (s/d)\")\n    user_input = input(\"Choice: \")\n    if user_input == \"s\":\n        history = json.load(open(\"history.json\"))\n        current_memory = json.load(open(\"memory.json\"))\n        history.append(current_memory)\n        json.dump(history, open(\"history.json\", \"w\"))\n        print(\"Current memory.json saved in history.json.\")        \n\n    elif user_input == \"d\":\n        print(\"Current memory.json discarded.\")\n        \n    memory = {\n        \"status\":\"inactive\",\n        \"objective\":\"\",\n        \"theory\":\"\",\n        \"theory_dossier\":\"\",\n        \"theory_assessment\":\"\",\n        \"theory_delegation\":\"\",\n        \"theory_scripts_gpt\":\"\",\n        \"theory_scripts_terminal\":\"\",\n        \"display_work\":\"\",\n        \"apply_theory\":\"\",\n        \"modify\":\"\",\n        \"modify_theory\":\"\",\n        \"archive_memory\":\"\",\n        \"execute_old_memory\":\"\" \n        }\n    json.dump(memory, open(\"memory.json\", \"w\"))        \n    return print (\"memory archived\")\n\ndef execute_memory():\n    print(\"Executing an old memory...\")\n    # pass memory to memory.json\n    # set status to memory.json\n    return status_memory()",
    "file_summary": "I'm sorry, but you didn't provide any specific Python file or content. Can you please provide the necessary Python file and content so I can summarize it for you?"
  },
  ".\\features\\query\\query_plugins\\query_chains\\f4_trajectory_and_actions.py": {
    "file_title": "f4_trajectory_and_actions.py",
    "file_path": ".\\features\\query\\query_plugins\\query_chains\\f4_trajectory_and_actions.py",
    "file_code": "import json\nimport sys\nimport os\nimport json\nos.chdir(os.path.dirname(os.path.abspath(__file__)))\nfrom f3_memory_management import status_memory, update_memory\n\ndef objective():\n    from core.f5_gpt_interpreter import gpt4_api\n    from core.f2_handbook_management import read_handbook\n    \n    # craft the prompt to send to gpt api\n    handbook_prompt = read_handbook(\"prompt\")\n    prefix=\"Please find the objective of this task:\\n\" # template\n    suffix=\"\\nStart your reply with 'The objective of this query is'\" # check how formatting influences output\n    api_prompt = (prefix + \" \" + handbook_prompt + suffix)\n    print(\"\\nFull prompt to send to api:\\n \",api_prompt)\n    gpt4_answer = gpt4_api(api_prompt)\n\n    # fill memory.json objective value with gpt4 answer\n    target = [\"objective\",gpt4_answer]\n    with open(\"memory.json\", \"r\") as file:\n        memory_data = json.load(file)\n    memory_data[target[0]] = target[1]\n    with open(\"memory.json\", \"w\") as file:\n        json.dump(memory_data, file)    \n    print(\"\\nMemory updated: \", target)\n\n    # update memory.json status to dossier\n    target = [\"status\",\"dossier\"]\n    update_memory(target) # update status\n    \n    return status_memory() \n\ndef dossier():\n    from f3_memory_management import status_memory\n    from f5_gpt_interpreter import gpt4_api\n    # craft the prompt to send to gpt api\n    memory_dossier = json.load(open(\"memory.json\"))[\"objective\"]\n    memory_dossier_str = json.dumps(memory_dossier) \n    #??? content? dossier_str = memory_data[\"dossier\"][\"content\"]\n    prefix=\"\"\"Please make a plan dossier for the objective. \nThe purpose of this dossier is to use the power of gpt4 and the computer terminal if one of these two are needed.\nThe dossier must contain to lists, one for openAI gpt calls and one for computer terminal commands. \nObjective:\n\"\"\"\n    suffix=\"\"\"Please answer in this format:\n\nGPT messages to send\n1.\n2.\n...\n\nTerminal commands for windows 11\n1.\n2.\n...'\"\"\" # check how formatting influences output\n    api_prompt = (prefix + memory_dossier_str + \"\\n\" + suffix )\n    print(\"\\nFull prompt to send to api:\\n \",api_prompt)\n    gpt4_answer = gpt4_api(api_prompt)\n\n    # fill memory.json dossier value with gpt4 answer\n    target = [\"dossier\",gpt4_answer]\n    with open(\"memory.json\", \"r\") as file:\n        memory_data = json.load(file)\n    memory_data[target[0]] = target[1]\n    with open(\"memory.json\", \"w\") as file:\n        json.dump(memory_data, file)    \n    print(\"\\nMemory updated: \", target)\n\n    # update memory.json status to \n    target = [\"status\",\"-\"]\n    update_memory(target) # update status\n    return exit()\n\n\n#########################################################\n\ndef execute_work():\n    from f3_memory_management import status_memory\n    memory = json.load(open(\"memory.json\"))[\"apply_theory\"]\n    # Output and actions to apply_theory#\n    # actions in theory do them\n    return status_memory()\n\ndef modify_work():\n    from f1_pilot import initialize_system\n    from f3_memory_management import status_memory\n    print(\"Execution halted, open 'memory.json' and apply changes. Write 'continue' to proceed or 'reboot' to start over.\")\n    user_input = input(\"Action: \")\n    if user_input == \"continue\":\n        print(\"Continuing...\")\n        memory = {\"status\": \"display_work\"}\n        json.dump(memory, open(\"memory.json\", \"w\"))\n        return status_memory()\n    \n    elif user_input == \"reboot\":\n        print(\"Rebooting...\")\n        memory = {\"status\": \"inactive\"}\n        json.dump(memory, open(\"memory.json\", \"w\"))        \n        return initialize_system()\n    \n    else:\n        print(\"Invalid choice. Exiting...\")\n        return",
    "file_summary": "It seems like you forgot to provide both the filename and the file content for me to review and summarize for you. Please provide the details so I can assist you better."
  },
  ".\\features\\query\\query_plugins\\query_chains\\f5_gpt_interpreter.py": {
    "file_title": "f5_gpt_interpreter.py",
    "file_path": ".\\features\\query\\query_plugins\\query_chains\\f5_gpt_interpreter.py",
    "file_code": "import json\nimport sys\nimport os\nimport json\nimport os\nimport openai\nfrom dotenv import load_dotenv\nos.chdir(os.path.dirname(os.path.abspath(__file__)))\nload_dotenv()\nopenai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\ndef handbook_system():\n    with open(\"handbook.json\", \"r\") as file:\n        handbook_data = json.load(file)\n    \n    system_data = handbook_data[\"system\"]\n    print(\"\\nhandbook_system:\",system_data)\n    return system_data\n\n\ndef gpt4_api(x):\n    # Make the OpenAI API call to get the response using GPT-4\n    # handle if situations to change the prompt to send to gpt4 besed on which value is \"status\" from memory.json\n    h_system = handbook_system()\n    completion = openai.ChatCompletion.create(\n    model=\"gpt-4\",\n    messages=[\n        {\"role\": \"system\", \"content\": h_system},\n        {\"role\": \"user\", \"content\": x}\n    ]\n    )\n\n\n    return completion.choices[0].message\n\n\n\n\n\n",
    "file_summary": "Without the actual Python file ('{file}') and its content ('{content}'), I can't provide a summary. Could you please provide the specific details?"
  },
  ".\\features\\query\\query_plugins\\query_chains\\f6_user_interaction.py": {
    "file_title": "f6_user_interaction.py",
    "file_path": ".\\features\\query\\query_plugins\\query_chains\\f6_user_interaction.py",
    "file_code": "import json\nimport sys\nimport os\nimport json\nos.chdir(os.path.dirname(os.path.abspath(__file__)))\nfrom f3_memory_management import status_memory\ndef display_work():\n    print(\"Do you want to see everything or theory_delegation? (all/td)\")\n    user_input = input(\"Choice: \")\n    if user_input == \"all\":\n        memory = json.load(open(\"memory.json\"))\n        print(memory)\n    elif user_input == \"td\":\n        memory = json.load(open(\"memory.json\"))\n        print(memory[\"theory_scripts_terminal\"] + memory[\"theory_scripts_gpt\"])\n    return user_feedback()\n\ndef user_feedback():\n    \n    print(\"Apply theory or modify something? (apply/modify)\")\n    user_input = input(\"Choice: \")\n    if user_input == \"apply\":\n        memory = {\"status\": \"apply_theory\"}\n        json.dump(memory, open(\"memory.json\", \"w\"))\n        return status_memory()\n\n    elif user_input == \"modify\":\n        memory = {\"status\": \"modify_theory\"}\n        json.dump(memory, open(\"memory.json\", \"w\"))\n        return status_memory()",
    "file_summary": "Apologies for the confusion, but the provided information lacks the necessary details for me to provide a summary. If you can provide the actual content of the Python file, I'd be happy to help!"
  },
  ".\\features\\query\\query_plugins\\query_chains\\f7_terminal_execution.py": {
    "file_title": "f7_terminal_execution.py",
    "file_path": ".\\features\\query\\query_plugins\\query_chains\\f7_terminal_execution.py",
    "file_code": "import json\nimport sys\nimport os\nimport json\nfrom f3_memory_management import status_memory\nos.chdir(os.path.dirname(os.path.abspath(__file__)))\ndef pilot_terminal():\n    # Execute terminal actions on the computer\n    # read the scripts from memory.json and execute them\n    return status_memory()",
    "file_summary": "It seems you have not provided the actual content of the Python file. Can you please provide the code that you'd like me to summarize?"
  },
  ".\\features\\query\\query_plugins\\query_chains\\temp.py": {
    "file_title": "temp.py",
    "file_path": ".\\features\\query\\query_plugins\\query_chains\\temp.py",
    "file_code": "import os\nimport json\nos.chdir(os.path.dirname(os.path.abspath(__file__)))\nfrom f3_memory_management import read_memory\n\nprint(read_memory(\"dossier\"))",
    "file_summary": "You haven't provided a Python file or its content for me to provide a brief summary. Please provide the necessary information so I can help you understand it better."
  },
  ".\\features\\query\\query_plugins\\query_chains\\__init__.py": {
    "file_title": "__init__.py",
    "file_path": ".\\features\\query\\query_plugins\\query_chains\\__init__.py",
    "file_code": "",
    "file_summary": "I'm sorry, but you didn't provide any specific Python file or its content to analyze and summarize. Please provide those details so that I can assist you effectively!"
  },
  ".\\introspection\\guided_introspection.py": {
    "file_title": "guided_introspection.py",
    "file_path": ".\\introspection\\guided_introspection.py",
    "file_code": "from typing import List\nfrom pathlib import Path\n\nimport os\nimport openai\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\ndef create_folder_structure_file(start_path: str, output_file: str) -> None:\n    with open(output_file, \"w\") as f:\n        for root, _, files in os.walk(start_path):\n            if \"__pycache__\" not in root and \".git\" not in root and \".misc\" not in root:\n                for file in files:\n                    path = os.path.join(root, file)\n                    f.write(f\"{path}\\n\")\n\ndef get_all_lines_from_file(file_path: Path) -> List[str]:\n    return file_path.read_text().splitlines()\n\ndef ask_user_option() -> str:\n    while True:\n        user_input = input(\"Do you want to consider a whole folder (type 'yes'), list the specific files (type 'no') or specify specific folder (type directory path)? \")\n        if user_input.lower() == \"yes\" or user_input.lower() == \"no\" or os.path.isdir(user_input):\n            return user_input\n        else:\n            print(\"Invalid input, please enter 'yes', 'no' or a valid directory path.\")\n\ndef get_files_to_consider() -> List[str]:\n    base_path = Path(__file__).absolute().parent\n    all_files = get_all_lines_from_file(base_path / \"1_folder_structure.txt\")\n    print(\"Files in the folder structure:\")\n    [print(f\"{i}. {_file}\") for i, _file in enumerate(all_files)]\n    return get_valid_user_input_for_files(all_files)\n\ndef get_valid_user_input_for_files(all_files: List[str]) -> List[str]:\n    while True:\n        user_input = input(\"\\nEnter file numbers to consider (comma-separated): \")\n        if is_valid_user_input(user_input, len(all_files)):\n            break\n        else:\n            print(\"Invalid input. Please enter valid file numbers separated by commas.\")\n    return [all_files[int(i)] for i in user_input.split(\",\")]\n\ndef is_valid_user_input(user_input: str, max_limit: int) -> bool:\n    return all(x.isdigit() for x in user_input.split(\",\")) and all(0 <= int(x) < max_limit for x in user_input.split(\",\"))\n\ndef save_code_files(files: List[str], output_file: str) -> None:\n    with output_file.open(\"w\") as f:\n        for _file in files:\n            try:\n                content = Path(_file).read_text().strip()\n                if content:\n                    f.write(f\"### {_file}\\n```\\n{content}\\n```\\n\")\n                else:\n                    f.write(f\"### {_file} (Empty)\\n\")\n            except FileNotFoundError as e:\n                print(f\"Error reading file '{_file}': {e}\")\n\ndef save_prompt(user_prompt: str, output_file: Path) -> None:\n    output_file.write_text(f\"## User Prompt\\n{user_prompt}\")\n\ndef merge_files(input_files: List[str], output_file: Path) -> None:\n  \n    base_path = Path(__file__).absolute().parent\n    with open(output_file, \"w\") as out_file:\n        out_file.write(\"\\n-----\\n\\n\".join([base_path.joinpath(_file).read_text().strip() \n                                          if base_path.joinpath(_file).exists() \n                                          else f\"{_file} (Empty)\"\n                                          for _file in input_files]))\n\ndef get_user_prompt(filepath: Path) -> str:\n    if filepath.exists():\n        last_prompt = filepath.read_text().strip()\n        if last_prompt:\n            user_input = input(f\"\\nLast used prompt: {last_prompt}\\nDo you want to reuse the last prompt? (Type 'reuse' or enter a new prompt): \").strip()\n            if user_input.lower() == \"reuse\":\n                return last_prompt\n    return input(\"\\nPlease enter the user prompt: \").strip()\n\ndef get_api_response(user_data: str) -> str:  \n    openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n\n\n    response = openai.ChatCompletion.create(\n       model=\"gpt-4\",\n       messages=[\n           {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n           {\"role\": \"user\", \"content\": user_data}\n       ]\n    )\n    return response.choices[0].message['content']\n\ndef guided_introspection():\n    base_path = Path(__file__).absolute().parent\n\n    # Section to ask user\n    user_choice = ask_user_option()\n\n    if user_choice.lower() == \"yes\":\n        start_folder = \".\"\n        is_whole_folder = True\n    elif user_choice.lower() == \"no\":\n        start_folder = \".\"\n        is_whole_folder = False\n    else:\n        start_folder = user_choice\n        is_whole_folder = True\n        \n    create_folder_structure_file(start_folder, base_path / \"1_folder_structure.txt\")\n    \n    # Check user's choice\n    if is_whole_folder:\n        # consider all files\n        all_files = get_all_lines_from_file(base_path / \"1_folder_structure.txt\")\n        files_to_consider = [file for file in all_files if \"__pycache__\" not in file and \".git\" not in file and \".misc\" not in file]\n        print(f\"All files ({len(files_to_consider)} total) are considered.\")\n    else:\n        # consider selected files\n        files_to_consider = get_files_to_consider()\n\n    save_code_files(files_to_consider, base_path / \"2_code.txt\")\n\n    prompt_file = base_path / \"3_prompt.txt\"\n    user_prompt = get_user_prompt(prompt_file)\n    save_prompt(user_prompt, prompt_file)\n\n    output_file = base_path / \"files_and_prompt.txt\"\n    merge_files([\"1_folder_structure.txt\", \"2_code.txt\", \"3_prompt.txt\"], output_file)\n    \n    api_response = get_api_response(output_file.read_text().strip())\n    with open(base_path / \"gpt_response.txt\", \"w\") as file:  \n        file.write(api_response)\n\ndef clean_files():\n    base_path = Path(__file__).absolute().parent\n    files_to_clean = [\"1_folder_structure.txt\", \"2_code.txt\", \"3_prompt.txt\", \"gpt_response.txt\", \"files_and_prompt.txt\"]\n\n    for filename in files_to_clean:\n        filepath = base_path / filename\n        if filepath.exists():\n            os.remove(filepath)\n\nif __name__ == \"__main__\":\n    clean_files()\n    guided_introspection()\n",
    "file_summary": "To provide an accurate summary, you'll need to give me the specific content of the Python file you need summarized. As per your instructions, you used placeholders '{file}' and '{content}' without providing their actual values. Please provide the Python code so I can assist you."
  },
  ".\\introspection\\__init__.py": {
    "file_title": "__init__.py",
    "file_path": ".\\introspection\\__init__.py",
    "file_code": "",
    "file_summary": "You didn't provide information on the Python file or the content. Could you please provide further details?"
  }
}